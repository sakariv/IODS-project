# Clustering and classification

## Loading the "Boston" data

The "Boston" dataset is derived from U.S. census information concerning housing in the suburbs of Boston Massachusetts. It is included in the 'MASS' library in R.

```{r}
# access the MASS package which has the data
library(MASS)

# load the data
data("Boston")

# exploring the data
str(Boston)

```

* The "Boston" dataset contains 506 observations and 14 continuous variables. 

* The variables (based on information at "https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html"):
  + 'crim' per capita crime rate by town.
  + 'zn' proportion of residential land zoned for lots over 25,000 sq.ft.
  + 'indus' proportion of non-retail business acres per town.
  + 'chas' Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
  + 'nox' nitrogen oxides concentration (parts per 10 million).
  + 'rm' average number of rooms per dwelling.
  + 'age' proportion of owner-occupied units built prior to 1940.
  + 'dis' weighted mean of distances to five Boston employment centres.
  + 'rad' index of accessibility to radial highways.
  + 'tax' full-value property-tax rate per $10,000.
  + 'ptratio' pupil-teacher ratio by town.
  + 'black' 1000(Bkâˆ’0.63)^2^ where Bk is the proportion of blacks by town.
  + 'lstat' lower status of the population (percent).
  + 'medv' median value of owner-occupied homes in $1000s.
  
## Graphical overview of the data

The dataset has quite many variables so visualization with 'plot' or 'pairs' functions makes it difficult to read

First we visualize the potential correlations between the variables in the dataset:

```{r}
# Graphical overview of the correlations in the data

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="square")

```

Some observations based on the correlation matrix:

* crime rate ('crim') is positively correlated with 'rad' and 'tax'
  + higher property taxes are positively correlated with higher crime rate, meaning city centers? Positive correlation with distance from radial highways would also point to that.
  + 'tax' and 'rad' are also highly correlated with each other
* proportion of residential land zoned for lots over 25,000 sq.ft.('zn') is negatively correlated with 
  + nonresidential
  + nitrogen gas emissions
  + age
  
Elaborate later on this....

```{r}



```

Standardize the dataset and print out summaries of the scaled data. How did the variables change? Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)

## Standardizing and scaling the variables

The dataset is scaled using 'scale' function

* subtract the column means from the corresponding columns and divide the difference with standard deviation.

$$scaled(x) = \frac{x - mean(x)}{ sd(x)}$$

```{r}
# Summary of the dataset before scaling
summary(Boston)

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

```

Scaling changed the data from original values to a z-score. The z-score describes how many standard deviations the a data point is from the mean.

## Creating a categorical variable for crime rate

We change the 'crim' to a categorical by breaking it into quantiles

```{r}
# Changing the dataset into a data frame:
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled$crim <- as.numeric(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

```

## Dividing the dataseto

For training and testing the classifier later, we divide the data into train (80%) and test (20%) datasets

```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

```

## Fitting the linear discriminatn analysis

We fit the linear discriminatn analysis on the training set with 'crime' as the categorical target variable. All the other variables are used as predictors.

```{r}

# linear discriminant analysis ("~ ." signifying that all other variables are predictors)
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "Maroon 2", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)

```

## Predicting the classes with the model

Save the crime categories from the test set and then remove the categorical crime variable from the test dataset. Then predict the classes with the LDA model on the test data. Cross tabulate the results with the crime categories from the test set. Comment on the results. (0-3 points)


```{r}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# Create the crosstabulation
prediction_tab <- table(correct = correct_classes, predicted = lda.pred$class)

# Calculate the row-wise percentages
row_percentages <- apply(prediction_tab, 1, prop.table) * 100

# Print the table with row-wise percentages
prediction_tab
row_percentages


```

The model seems to be able to predict the test data rather well. I does the best with 'high' category with 100% accuracy and the worst with medium high category (50%). The misses seem to be "clustered" around the correct category.

## Distance measures with k-means analysis 

```{r}
data("Boston")
summary(Boston)

#scaling the dataset
boston_scaled <- scale(Boston)

# looks weird
str(boston_scaled)

# the scaled dataset needs to be a data frame for 'pairs' function to work
boston_scaled <- as.data.frame(boston_scaled)

# now it looks better..
str(boston_scaled)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# k-means clustering
km <- kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled[c("rm", "age", "dis", "crim")], col = km$cluster)

```
In Exercise 4, the nonscaled 'Boston' data was used. In comparison to the exercise set, scaling the data did not change the outlook of the plots. it did change the scale to z-scores, representing how many standard deviations the mean.

## The optimal number of clusters


```{r}
library(ggplot2)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
As the line drops drastically at 2, we use this for the optimum number of clusters

```{r}

# k-means clustering
km <- kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled[c("rm", "age", "dis", "crim")], col = km$cluster)


```
