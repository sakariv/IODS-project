# Clustering and classification

## Loading the "Boston" data

The "Boston" dataset is derived from U.S. census information on housing values in the suburbs of Boston Massachusetts. It is included in the 'MASS' library in R.

```{r}
# access the MASS package which has the data
library(MASS)

# loading the data
data("Boston")

# exploring the data
str(Boston)

```

* The "Boston" dataset contains 506 observations (suburbs or towns) and 14 continuous numerical variables. 

* The variables (based on information at "https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html"):
  + 'crim' per capita crime rate by town.
  + 'zn' proportion of residential land zoned for lots over 25,000 sq.ft.
  + 'indus' proportion of non-retail business acres per town.
  + 'chas' Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).
  + 'nox' nitrogen oxides concentration (parts per 10 million).
  + 'rm' average number of rooms per dwelling.
  + 'age' proportion of owner-occupied units built prior to 1940.
  + 'dis' weighted mean of distances to five Boston employment centres.
  + 'rad' index of accessibility to radial highways.
  + 'tax' full-value property-tax rate per $10,000.
  + 'ptratio' pupil-teacher ratio by town.
  + 'black' 1000(Bkâˆ’0.63)^2^ where Bk is the proportion of blacks by town.
  + 'lstat' lower status of the population (percent).
  + 'medv' median value of owner-occupied homes in $1000s.

In this assignment we will focus on the crime rate ('crim') as the dependent variable.

## Graphical overview of the data

Summaries to explore the variables: 

```{r}
# summaries of the variables
summary(Boston)
```

Histograms to visualize the distributions of the different variables:

```{r}
# Loading the library
library(ggplot2)

# Create a list of variable names
variables <- names(Boston)

# Loop through the variables and create a histogram for each one
for (var in variables) {
  # Create the histogram
  p <- ggplot(Boston, aes_string(x = var)) + 
    geom_histogram(binwidth = 1) +
    labs(x = var, y = "Frequency", title = paste("Histogram of", var))
  
  # Print the histogram
  print(p)
}

# separate histogram for 'nox' as it is very poorly visualized with binwith of 1
 ggplot(Boston, aes(x = nox)) + 
    geom_histogram(binwidth = 0.1) +
    labs(x = var, y = "Frequency", title = paste("Histogram of nox binwidth 0.1"))


```

Some general observations on variable distributions: 

* The binwith 1 is probably not optimal for visualizing all of the variables but at least it shows something about every variable.
  + for nox a separate histogram with binwidth of 0.1
* The variables vary quite a bit regardign min/max and their distributions, very few are normally distributed (maybe only 'rm')
* 'chas' is a binary dummy variable

Some observations about individual variables:

* 'crim': Based on the histogram, most of the boroughs/ towns have a low crime rate, with a few with a very high crime rate. This is also reflected by the median being a lot lower than the mean.
* 'zn': most of the boroughs/towns have no residential land zoned for lots over 25,000 sq.ft.
* 'age': based on the data, there are a lot of districts, where all of the owner-occupied units were built before 1940.


The correlations between the variables can be visualized using corrplot function:

```{r}
# Graphical overview of the correlations in the data

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="square")

```

Some observations based on the correlation matrix:

Our dependent variable 'crim' (per capita crime rate by town):
* crime rate ('crim') is positively highly correlated with 'rad' and 'tax'
  + higher property taxes are positively correlated with higher crime rate, meaning city centers? Positive correlation with distance from radial highways would also point to that.
  + 'tax' and 'rad' are also highly correlated with each other, so they might not be directly related to 'crim', but could be properties of city centres

Observations on some other variables:

* nitrogen gas emissions ('nox') are positively correlated with 'ind' (non-retail businesses) which makes sense
* 'nox' is also correlated with 'tax', caused by car emissions in city centre??
  + 'nox' also negatively correlated with 'dis' (distance from employment centres) 
* 'age' is negatively correlated with 'dis' so newer houses in the suburbs?
  


Create a categorical variable of the crime rate in the Boston dataset (from the scaled crime rate). Use the quantiles as the break points in the categorical variable. Drop the old crime rate variable from the dataset. Divide the dataset to train and test sets, so that 80% of the data belongs to the train set. (0-2 points)
## Standardizing and scaling the variables

The dataset is scaled using 'scale' function

* subtract the column means from the corresponding columns and divide the difference with standard deviation.

$$scaled(x) = \frac{x - mean(x)}{ sd(x)}$$

```{r}
# Summary of the dataset before scaling
summary(Boston)

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)

```

Scaling changed the data from original values to a z-score. The z-score describes how many standard deviations the a data point is from the mean.

## Creating a categorical variable for crime rate

We change the 'crim' to a categorical by breaking it into quantiles: 

```{r}
# Transforming the dataset into a data frame:
boston_scaled <- as.data.frame(scale(Boston))
boston_scaled$crim <- as.numeric(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

# remove original 'crim' from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# checking if the 'crim' is out and 'crime' is in
colnames(boston_scaled)

# seems ok
```

## Dividing the dataset into train and test sets

For training and testing the classifier later, we divide the data into train (80%) and test (20%) datasets:

```{r}

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

```

## Fitting the linear discriminatn analysis

We fit the linear discriminant analysis on the training set with 'crime' as the categorical target variable. All the other variables are used as predictors:

```{r}

# linear discriminant analysis ("~ ." signifying that all other variables are predictors)
lda.fit <- lda(crime ~ ., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "Maroon 2", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2)
lda.arrows(lda.fit, myscale = 1)

```


## Predicting the classes with the model

We use the LDA model to predict 'crime' in the test data set:

```{r}

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# Create the crosstabs
prediction_tab <- table(correct = correct_classes, predicted = lda.pred$class)

# Calculate the row-wise percentages
row_percentages <- apply(prediction_tab, 1, prop.table) * 100

# Print the table with row-wise percentages
prediction_tab
row_percentages


```

The model seems to be able to predict to which of the four crime rate categories the data points fall in the test data rather well. I does the best with 'high' category with 100% accuracy and the worst with medium high category (50%). 

## Distance measures with k-means analysis 

```{r}
data("Boston")
summary(Boston)

#scaling the dataset
boston_scaled <- scale(Boston)

# looks weird
str(boston_scaled)

# the scaled dataset needs to be a data frame for 'pairs' function to work
boston_scaled <- as.data.frame(boston_scaled)

# now it looks better..
str(boston_scaled)

# euclidean distance matrix
dist_eu <- dist(boston_scaled)

# look at the summary of the distances
summary(dist_eu)

# k-means clustering
km <- kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled[c("rm", "age", "dis", "crim")], col = km$cluster)

```
In Exercise 4, the nonscaled 'Boston' data was used. In comparison to the exercise set, scaling the data did not change the outlook of the plots. it did change the scale to z-scores, representing how many standard deviations the observations are from the mean.



## The optimal number of clusters

An attempt on the first bonus task..:


```{r}
library(ggplot2)
load(Boston)

#scaling the data
boston_bonus <- scale(Boston)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```
As the line drops drastically at 2, we use this for the optimum number of clusters

```{r}

# k-means clustering
km <- kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled[c("rm", "age", "dis", "crim")], col = km$cluster)


```
