# Regression and model validation

## Week 2: learning 

* Using R is quite new to me, so lot of new stuff...
* I studied simple regression from chapter 7 in  "R for Data Science" and by doing the Exercise 2
* I learned some more R markdown syntax while writing Chapter 2

* Assignment 2
  + Data wrangling: importing the data and arranging it based on Exercise 2
  + Data analysis: Based on Excercise 2 and chapter 7 in  "R for Data Science"

Note: if I work from home with work computer, I need to have VPN on in order to push to github. Otherwise I get a proxy error

## Data wrangling

A link to my R script in the GitHub repository:
https://github.com/sakariv/IODS-project/blob/master/data/create_learning2014.R

## Data analysis 

### Importing and exploring the data

```{r}
# Reading the data from local folder:
library(readr)
learning2014 <- read_csv("data/learning2014.csv")

# Exploration of the structure the data
str(learning2014)

# Exploration of the dimensions of the data
dim(learning2014)
```
Description of the data set:

* The 'learning2014' data set has 166 observations.

* The data set is comprised of answers of student survey, some background information ('gender', 'age') and their course performance ('points'). In data wrangling, the number of variables was reduced from from 60 to 7. 

* In the wrangled data the variable "attitude" was left as such and the following combination variables were created: 

  + 'deep', questions related to deep learning
  + 'stra', questions related to strategic learning
  + 'surf', questions related to surface level learning

* The observations where "points" was 0 were removed from the dataset.

### Summary of the data

```{r}
# Overview of the data

summary(learning2014)

# Summary of gender distribution

table(learning2014$gender)

```
* The summary shows that the 'age' of the participants is skewed to lower side, which makes sense for student survey data. 

* The summary does not show data on character variable 'gender', but with 'table' function it was possible to see that there are almost double the amount of female students than male students represented in the data.

### Graphical overview of the data

```{r}
# accessing the necessary packages for plotting the data

library(GGally)
library(ggplot2)
library(tidyverse)

# plotting the graphical overview of the data

ggpairs(learning2014, mapping = aes(), lower = list(combo = wrap("facethist", bins = 20)))
```

* Based on the plotted data there are following significant correlations:

  + surface level learning ('surf') is negatively correlated with 'attitude', deep learning ('deep'), and strategic learning ('stra')
  + points is positively correlated with 'attitude' 

* Other than 'age', the distributions of variables don't seem terribly skewed from normal distribution.

### Variables and a regression model

* The following explanatory variables were chosen for the model as they had the highest correlations with 'points' based on the plotted data:

  + 'attitude'
  + 'stra'
  + 'surf'

```{r}
# creating a regression model with the variables attitude, stra and surf

lrn2014_model <- lm(points ~ attitude + stra + surf, data = learning2014)

summary(lrn2014_model)

```
The intercept = 'points' when all explanatory variables are 0

Slopes:

  + Each point in 'attitude' improves the points by 3.4 in the model
  + Each point in 'stra' improves the points by 0.9 in the model
  + Each point in 'surf' decreases the points by 0.8 in the model

* Based on the multiple R-squared, 21% of the variation of the dependent variables are explained by the chosen explanatory variables.

* The F-statistic is statistically significant, meaning that at least one of the explanatory variables are contributing significatly to explaining the variablilty of the independent variable.

## The new model

Only the variable 'attitude' had statistically significant relationship (slope) with the dependent variable 'points'. Therefore, only it was left in the final model as an explanatory variable:
```{r}
# New model with only "attitude" the explanatory variable

lrn2014_model_adjusted <- lm(points ~ attitude, data = learning2014)

```

Summary of the new model
```{r}
# Summary of the model with 'attitude' as the only explanatory variable

summary(lrn2014_model_adjusted)

```

* Now with attitude as the only explanatory variable in the model, based on the multiple R squared it explains 19% of the variation. This is a 2 percentage point drop from including 'surf' and 'stra', for which the correlation was not statistically significant
* Intercept: The points when explanatory variable = 0
  + based on the model, when explanatory variable 'attitude' = 0, 'points' = 11.6 
* Slope: A 'points' increase by 3.5 for each point in 'attitude'

### Diagnostic plots

Linear regression model has the following assumptions:

* Linear relationship between the predictor (explanatory variable) and outcome (dependent variable)
* Independence of residuals
  + the observations need to be independent from each other
  + more often a problem in time series data
  + some relationship in the data that is not identified by the model 
* Normal distribution of residuals
  + the residuals should show a normal distribution with a mean of zero
* Equal variance of residuals
  + the distance of the observations need to be the same on the both sides of the line
  
```{r}
# Creating the diagnostic plots for the model with 'points' as the dependent and 'attitude' as the explanatory variable 
plot(lrn2014_model_adjusted, which = c(1,2,5))

```

The diagnostic plots test the following assumptions:

* The Residuals vs fitted plot tests for linearity and equal variances of the residuals
  + The plot indicates linearity, as there is no apparent "arc" formed by the fitted values
  + The plot indicates some unequal variances of the residuals, as they fan out slightly towards the left. This violates the assumptions of linear regression

* The Q-Q plot tests for normality assumption
  + The points fall to the line, which indicates normality

* The Residuals vs Leverage plot tests for influential observations (outliers) in the model 
  + The plot does not indicate outliers, as none of the observations are outside the Cook's distance lines
  + The plot indicates unequal variances, which violates the assumptions of the linear regression model